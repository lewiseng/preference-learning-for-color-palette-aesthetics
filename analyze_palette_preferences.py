"""
End-to-end analysis pipeline for MTurk color-palette preference modeling.

This module:
    1) Loads MTurk palette ratings from a .mat file.
    2) Engineers RGB/Lab/HSV/CHSV palette features (including hue-probability stats).
    3) Trains a rating-based baseline (LASSO regression) on user-normalized targets.
    4) Trains a Bradley–Terry style logistic model on synthetic pairwise preferences.
    5) Evaluates both models on shared pairwise comparisons and visualizes selected palettes.
    6) Repeats the experiment across multiple random seeds, reporting mean ± std metrics
       and a paired t-test comparing BT vs LASSO pairwise accuracy.

The code in this file is structured to support research-style experiments on palette
preference prediction and feature importance analysis.
"""

import os

import numpy as np
import pandas as pd

# Ensure Matplotlib can write its config/cache in restricted environments
_MODULE_DIR = os.path.dirname(__file__)

# Ensure Matplotlib can write its config/cache in restricted environments
_mplconfig_dir = os.path.join(_MODULE_DIR, ".mplconfig")
os.environ.setdefault("MPLCONFIGDIR", _mplconfig_dir)

# Central location for all PNG outputs generated by this script
OUTPUT_DIR = os.path.join(_MODULE_DIR, "output")
os.makedirs(OUTPUT_DIR, exist_ok=True)

import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib.colors import rgb_to_hsv as mpl_rgb_to_hsv
from sklearn.model_selection import train_test_split  # noqa: F401 (imported for future use)
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.metrics import mean_squared_error, roc_auc_score
from scipy.stats import ttest_rel
from scipy.io import loadmat
from scipy.interpolate import CubicSpline
from scipy.special import i0
from typing import Dict, List, Optional, Tuple

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# Default number of top-weighted Mturk features to keep when using Mturk weights
TOP_K_MTURK_FEATURES = 20


def loadData(matPath: str = "data/mturkData.mat") -> pd.DataFrame:
    """
    Load MTurk color palette ratings from a .mat file into a DataFrame.

    The resulting DataFrame contains all variables from the MAT file,
    with the following columns:
    - ids
    - names
    - targets (original mean ratings from the file)
    - userNormalizedTargets
    - color1_r, color1_g, color1_b, ..., color5_r, color5_g, color5_b
    """
    matData = loadmat(matPath)

    # Extract core arrays
    targets = matData.get("targets")
    ids = matData.get("ids")
    names = matData.get("names")
    data = matData.get("data")
    userNormalizedTargets = matData.get("userNormalizedTargets")

    if targets is None or ids is None or names is None or data is None:
        raise ValueError("Expected keys ('targets', 'ids', 'names', 'data') not found in MAT file.")

    nSamples = targets.shape[0]

    # Flatten simple column vectors
    targetsFlat = targets.reshape(nSamples)
    idsFlat = ids.reshape(nSamples)
    userNormFlat = userNormalizedTargets.reshape(nSamples) if userNormalizedTargets is not None else None

    # Convert names (object array of 1-element arrays) to plain strings
    nameList = []
    for row in names:
        # Each row is something like [array(['catamaran'], dtype='<U9')]
        cell = row[0]
        if isinstance(cell, np.ndarray):
            nameList.append(str(cell[0]))
        else:
            nameList.append(str(cell))

    # Flatten color data: shape (N, 5, 3) -> columns color1_r ... color5_b
    if data.ndim != 3 or data.shape[1] != 5 or data.shape[2] != 3:
        raise ValueError(f"Unexpected 'data' shape {data.shape}; expected (N, 5, 3).")

    colorCols = {}
    channelNames = ["r", "g", "b"]
    nColors = data.shape[1]
    for colorIdx in range(nColors):
        for chIdx, chName in enumerate(channelNames):
            colName = f"color{colorIdx + 1}_{chName}"
            colorCols[colName] = data[:, colorIdx, chIdx]

    # Build DataFrame with all information
    dfDict = {
        "ids": idsFlat,
        "names": nameList,
        "targets": targetsFlat,
    }

    if userNormFlat is not None:
        dfDict["userNormalizedTargets"] = userNormFlat

    dfDict.update(colorCols)

    df = pd.DataFrame(dfDict)
    return df


# ---------------------------------------------------------------------------
# Color-space utilities
# ---------------------------------------------------------------------------

def _rgb_to_lab(rgb: np.ndarray) -> np.ndarray:
    """
    Convert RGB colors to Lab, following the MATLAB RGB2Lab.m implementation.

    Args:
        rgb: Array of shape (3, num_colors) or (num_colors, 3).

    Returns:
        Lab array with shape (3, num_colors).
    """
    arr = np.asarray(rgb, dtype=float)
    if arr.ndim != 2 or 3 not in arr.shape:
        raise ValueError(f"Expected RGB array with one dimension of size 3, got {arr.shape}.")

    if arr.shape[0] == 3:
        r, g, b = arr[0], arr[1], arr[2]
    else:
        r, g, b = arr[:, 0], arr[:, 1], arr[:, 2]

    if np.max([r.max(), g.max(), b.max()]) > 1.0:
        r = r / 255.0
        g = g / 255.0
        b = b / 255.0

    r = r.reshape(1, -1)
    g = g.reshape(1, -1)
    b = b.reshape(1, -1)

    rgb_stack = np.vstack([r, g, b])

    mat = np.array(
        [
            [0.412453, 0.357580, 0.180423],
            [0.212671, 0.715160, 0.072169],
            [0.019334, 0.119193, 0.950227],
        ]
    )
    xyz = mat @ rgb_stack

    x = xyz[0, :] / 0.950456
    y = xyz[1, :]
    z = xyz[2, :] / 1.088754

    T = 0.008856

    def f(t: np.ndarray) -> np.ndarray:
        mask = t > T
        ft = np.empty_like(t)
        ft[mask] = np.cbrt(t[mask])
        ft[~mask] = 7.787 * t[~mask] + 16.0 / 116.0
        return ft

    fx = f(x)
    fy = f(y)
    fz = f(z)

    y3 = np.cbrt(y)
    yt = y > T
    L = np.empty_like(y)
    L[yt] = 116.0 * y3[yt] - 16.0
    L[~yt] = 903.3 * y[~yt]

    a = 500.0 * (fx - fy)
    b_lab = 200.0 * (fy - fz)

    L = L.reshape(1, -1)
    a = a.reshape(1, -1)
    b_lab = b_lab.reshape(1, -1)

    lab = np.vstack([L, a, b_lab])

    lab = lab / np.array([[100.0], [128.0], [128.0]])
    return lab


def _rgb_to_hsv(rgb: np.ndarray) -> np.ndarray:
    """
    Convert RGB colors to HSV using matplotlib, matching MATLAB rgb2hsv behavior.

    Args:
        rgb: Array of shape (3, num_colors) or (num_colors, 3).

    Returns:
        HSV array with shape (3, num_colors) in [0, 1].
    """
    arr = np.asarray(rgb, dtype=float)
    if arr.ndim != 2 or 3 not in arr.shape:
        raise ValueError(f"Expected RGB array with one dimension of size 3, got {arr.shape}.")

    if arr.shape[0] == 3:
        arr = arr.T

    if arr.max() > 1.0:
        arr = arr / 255.0

    hsv = mpl_rgb_to_hsv(arr)
    return hsv.T


# ---------------------------------------------------------------------------
# Geometric utilities
# ---------------------------------------------------------------------------

def _get_plane_features(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:
    """
    Python equivalent of getPlaneFeatures.m.

    Args:
        X: Array of shape (num_points, 3) representing coordinates.

    Returns:
        normal: Plane normal vector (3,).
        pct_explained: Variance explained by each principal component (3,).
        mean_x: Mean of points (3,).
        sse: Sum of squared errors to the fitted plane.
    """
    if X.ndim != 2 or X.shape[1] != 3:
        raise ValueError(f"Expected X with shape (num_points, 3), got {X.shape}.")

    mean_x = X.mean(axis=0, keepdims=True)
    Xc = X - mean_x

    _, S, Vt = np.linalg.svd(Xc, full_matrices=False)
    PC = Vt.T

    normal = PC[:, 2]
    if normal[0] < 0:
        normal = -normal

    roots = S**2
    if roots.sum() == 0:
        pct_explained = np.zeros(3, dtype=float)
    else:
        pct_explained = roots / roots.sum()

    mean_x_vec = mean_x.reshape(-1)
    error = np.abs((X - mean_x_vec) @ normal)
    sse = float((error**2).sum())
    return normal, pct_explained, mean_x_vec, sse


# ---------------------------------------------------------------------------
# Hue statistics and circular features
# ---------------------------------------------------------------------------

def _load_hue_probs_and_mapping(
    base_path: str = "data/data",
) -> Tuple[object, CubicSpline]:
    """
    Load hueProbsRGB.mat (hueProbs struct) and kulerX.mat (x vector) and build
    the hue remapping spline, matching the MATLAB code:
        load hueProbsRGB
        load kulerX
        y = (0:360)./360;
        mapping = spline(x, y);
    """
    hue_mat = loadmat(f"{base_path}/hueProbsRGB.mat", struct_as_record=False, squeeze_me=True)
    if "hueProbs" not in hue_mat:
        raise KeyError("Expected variable 'hueProbs' in hueProbsRGB.mat.")
    hue_probs = hue_mat["hueProbs"]

    kuler_mat = loadmat(f"{base_path}/kulerX.mat", struct_as_record=False, squeeze_me=True)
    if "x" not in kuler_mat:
        raise KeyError("Expected variable 'x' in kulerX.mat.")
    x = np.asarray(kuler_mat["x"], dtype=float).reshape(-1)

    # In the original MATLAB code the call is:
    #   y = (0:360)./360;
    #   mapping = spline(x, y);
    # MATLAB's `spline` is more permissive about nearly‑monotone `x` than
    # SciPy's CubicSpline, which requires a strictly increasing sequence.
    # The kulerX.mat values are almost, but not perfectly, monotone; to
    # satisfy CubicSpline we sort (x, y) by x before constructing the spline.
    y = np.linspace(0.0, 1.0, 361)
    sort_idx = np.argsort(x)
    x_sorted = x[sort_idx]
    y_sorted = y[sort_idx]
    mapping = CubicSpline(x_sorted, y_sorted)
    return hue_probs, mapping


def _compute_color_spaces(rgb: np.ndarray, mapping: CubicSpline) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Python equivalent of getColorSpaces.m (including hue remapping).

    Args:
        rgb: Array of shape (3, num_colors).
        mapping: CubicSpline used to remap hue.

    Returns:
        hsv, lab, chsv arrays (each 3 x num_colors).
    """
    hsv = _rgb_to_hsv(rgb)
    lab = _rgb_to_lab(rgb)

    hsv_remap = hsv.copy()
    hsv_remap[0, :] = mapping(hsv_remap[0, :])

    angles = 2.0 * np.pi * hsv_remap[0, :]
    chsv = np.vstack(
        [
            hsv_remap[1, :] * np.cos(angles),
            -hsv_remap[1, :] * np.sin(angles),
            hsv_remap[2, :],
        ]
    )

    return hsv, lab, chsv


def _get_basic_stats(x: np.ndarray, add_log: bool = True) -> np.ndarray:
    """
    Python version of getBasicStats.m.

    Computes simple summary statistics (mean, std, min, max) in both
    linear and log space, with basic protection against numerical issues.
    """
    x = np.asarray(x, dtype=float).reshape(-1)
    if x.size == 0:
        return np.zeros(8, dtype=float)

    # A small epsilon keeps log well-defined even when x contains zeros.
    log_x = np.log(x + 1e-6)
    features = np.array(
        [
            x.mean(),
            x.std(),
            x.min(),
            x.max(),
            log_x.mean(),
            log_x.std(),
            log_x.min(),
            log_x.max(),
        ],
        dtype=float,
    )
    features[~np.isfinite(features)] = 0.0
    return features


def _circ_vmpdf(alpha: np.ndarray, thetahat: float, kappa: float) -> np.ndarray:
    """
    Python implementation of circ_vmpdf.m using scipy.special.i0.
    """
    alpha = np.asarray(alpha, dtype=float).reshape(-1)
    C = 1.0 / (2.0 * np.pi * i0(kappa))
    return C * np.exp(kappa * np.cos(alpha - thetahat))


def _get_hue_prob_features(hsv: np.ndarray, sat_val_thresh: float, hue_probs: object) -> np.ndarray:
    """
    Python equivalent of getHueProbFeatures.m.
    """
    hsv = np.asarray(hsv, dtype=float)
    if hsv.shape[0] != 3:
        raise ValueError(f"Expected hsv with shape (3, num_colors), got {hsv.shape}.")

    # Restrict attention to “visible” hues with sufficient saturation and value
    select_colors = np.min(hsv[1:3, :], axis=0) >= sat_val_thresh

    scale = np.array([[359.0], [100.0], [100.0]])
    hsv2 = np.round(hsv * scale).astype(int) + 1
    vis_hues = hsv2[0, select_colors]

    hue_joint_list: List[float] = []
    hue_adj_list: List[float] = []

    if vis_hues.size > 0:
        hue_prob_arr = np.asarray(hue_probs.hueProb)
        hue_joint_arr = np.asarray(hue_probs.hueJoint)
        hue_adj_arr = np.asarray(hue_probs.hueAdjacency)

        for h1_idx in range(vis_hues.size):
            for h2_idx in range(h1_idx, vis_hues.size):
                i1 = vis_hues[h1_idx] - 1
                i2 = vis_hues[h2_idx] - 1
                hue_joint_list.append(float(hue_joint_arr[i2, i1]))

        for h1_idx in range(vis_hues.size - 1):
            i1 = vis_hues[h1_idx] - 1
            i2 = vis_hues[h1_idx + 1] - 1
            hue_adj_list.append(float(hue_adj_arr[i1, i2]))

        hue_prob_features = _get_basic_stats(hue_prob_arr[vis_hues - 1], add_log=True)
        hue_joint_prob_features = _get_basic_stats(np.asarray(hue_joint_list), add_log=True)
        hue_adj_prob_features = _get_basic_stats(np.asarray(hue_adj_list), add_log=True)
    else:
        hue_prob_features = np.zeros(8, dtype=float)
        hue_joint_prob_features = np.zeros(8, dtype=float)
        hue_adj_prob_features = np.zeros(8, dtype=float)

    alpha = np.linspace(0.0, 2.0 * np.pi, 361)[:-1]
    p_mix = 0.001 * np.ones_like(alpha)
    # Build a mixture of von Mises components centered at each visible hue
    for h in vis_hues:
        p_mix = p_mix + _circ_vmpdf(alpha, float(h) * 2.0 * np.pi / 360.0, 2.0 * np.pi)

    if vis_hues.size != 0:
        p_mix = p_mix / p_mix.sum()
        entropy = float(-(p_mix * np.log(p_mix + 1e-12)).sum())
    else:
        entropy = 5.9

    return np.concatenate([hue_prob_features, hue_joint_prob_features, hue_adj_prob_features, [entropy]])


# ---------------------------------------------------------------------------
# Feature-name construction and feature engineering
# ---------------------------------------------------------------------------

def _build_feature_names_for_space(
    name: str, n_colors: int
) -> Dict[str, List[str]]:
    """
    Construct feature-name lists mirroring the MATLAB createFeaturesFromData.m logic.
    """
    names: Dict[str, List[str]] = {}

    col_labels: List[str] = []
    for j in range(1, n_colors + 1):
        for d in range(1, 4):
            col_labels.append(f"{name}-D{d}-C{j}")
    names[f"{name}Col"] = col_labels

    sorted_col_labels: List[str] = []
    for j in range(1, n_colors + 1):
        for d in range(1, 4):
            sorted_col_labels.append(f"{name}Sorted-D{d}-C{j}")
    names[f"{name}SortedCol"] = sorted_col_labels

    diff_labels: List[str] = []
    for d in range(1, 4):
        for j in range(1, n_colors):
            diff_labels.append(f"{name}Diff-D{d}-C{j}")
    names[f"{name}Diff"] = diff_labels

    sorted_diff_labels: List[str] = []
    for d in range(1, 4):
        for j in range(1, n_colors):
            sorted_diff_labels.append(f"{name}SortedDiff-D{d}-C{j}")
    names[f"{name}SortedDiff"] = sorted_diff_labels

    names[f"{name}Mean"] = [f"{name}Mean-D{d}" for d in range(1, 4)]
    names[f"{name}StdDev"] = [f"{name}StdDev-D{d}" for d in range(1, 4)]
    names[f"{name}Median"] = [f"{name}Median-D{d}" for d in range(1, 4)]
    names[f"{name}Max"] = [f"{name}Max-D{d}" for d in range(1, 4)]
    names[f"{name}Min"] = [f"{name}Min-D{d}" for d in range(1, 4)]
    names[f"{name}MaxMinDiff"] = [f"{name}MaxMinDiff-D{d}" for d in range(1, 4)]

    if name != "hsv":
        names[f"{name}Plane"] = [
            f"{name}PlaneNormal1",
            f"{name}PlaneNormal2",
            f"{name}PlaneNormal3",
            f"{name}PlaneVariance-D1",
            f"{name}PlaneVariance-D2",
            f"{name}PlaneVariance-D3",
            f"{name}SSE",
        ]
    else:
        prefix_names = []
        for prefix in [f"{name}HueProb", f"{name}HueJointProb", f"{name}HueAdjProb"]:
            prefix_names.extend(
                [
                    f"{prefix}Mean",
                    f"{prefix}StdDev",
                    f"{prefix}Min",
                    f"{prefix}Max",
                    f"{prefix}LogMean",
                    f"{prefix}LogStdDev",
                    f"{prefix}LogMin",
                    f"{prefix}LogMax",
                ]
            )
        prefix_names.append(f"{name}Entropy")
        names[f"{name}HueProb"] = prefix_names

    return names


def create_features_from_data_array(
    data: np.ndarray,
    max_features: Optional[int] = None,
    sat_val_thresh: float = 0.2,
    aux_base_path: str = "data/data",
) -> Tuple[Dict[str, np.ndarray], Dict[str, List[str]], int, np.ndarray, np.ndarray]:
    """
    Python translation of createFeaturesFromData.m for use on mturkData.mat `data`.

    This ports the RGB/Lab/HSV/CHSV feature families (colors, sorted colors,
    diffs, sorted diffs, basic stats, plane features), and hue-probability
    features using hueProbsRGB.mat and kulerX.mat.

    Args:
        data: Array of shape (N, 5, 3) with palette colors.
        max_features: Optional cap on number of rows to process.
        sat_val_thresh: Threshold used in the circular hue-difference logic.
        aux_base_path: Directory containing hueProbsRGB.mat and kulerX.mat.

    Returns:
        all_features: dict mapping feature-group name -> (num_rows, num_features_in_group).
        feature_names: dict mapping feature-group name -> list of column labels.
        num_themes: number of rows actually processed.
        rgbs: flattened RGB values per row (num_rows, 15).
        labs: flattened Lab values per row (num_rows, 15).
    """
    if data.ndim != 3 or data.shape[1] != 5 or data.shape[2] != 3:
        raise ValueError(f"Expected data with shape (N, 5, 3), got {data.shape}.")

    hue_probs, mapping = _load_hue_probs_and_mapping(aux_base_path)

    n_samples = data.shape[0]
    num_themes = n_samples if max_features is None else min(int(max_features), n_samples)
    n_colors = data.shape[1]

    rgbs = np.zeros((num_themes, 3 * n_colors), dtype=float)
    labs = np.zeros((num_themes, 3 * n_colors), dtype=float)

    rgb_mats: List[np.ndarray] = []
    hsv_mats: List[np.ndarray] = []
    lab_mats: List[np.ndarray] = []
    chsv_mats: List[np.ndarray] = []
    num_colors_list: List[int] = []

    for i in range(num_themes):
        rgb_full = data[i].T
        mask_nonneg = rgb_full[0, :] >= 0
        num_colors_i = int(mask_nonneg.sum())
        if num_colors_i == 0:
            num_colors_i = rgb_full.shape[1]
        rgb = rgb_full[:, :num_colors_i]

        hsv, lab, chsv = _compute_color_spaces(rgb, mapping)

        rgb_mats.append(rgb)
        hsv_mats.append(hsv)
        lab_mats.append(lab)
        chsv_mats.append(chsv)
        num_colors_list.append(num_colors_i)

        rgbs[i, : 3 * num_colors_i] = rgb.reshape(-1, order="F")
        labs[i, : 3 * num_colors_i] = lab.reshape(-1, order="F")

    all_features: Dict[str, np.ndarray] = {}
    feature_names: Dict[str, List[str]] = {}

    dummy_hsv = np.random.rand(3, n_colors)
    dummy_hue_feats = _get_hue_prob_features(dummy_hsv, sat_val_thresh, hue_probs)
    hue_feat_len = dummy_hue_feats.shape[0]

    for name in ["chsv", "lab", "hsv", "rgb"]:
        feature_names.update(_build_feature_names_for_space(name, n_colors))

        color_arr = np.zeros((num_themes, 3 * n_colors), dtype=float)
        sorted_col_arr = np.zeros((num_themes, 3 * n_colors), dtype=float)

        diff_arr = np.zeros((num_themes, 3 * (n_colors - 1)), dtype=float)
        sorted_diff_arr = np.zeros((num_themes, 3 * (n_colors - 1)), dtype=float)

        means_arr = np.zeros((num_themes, 3), dtype=float)
        stddevs_arr = np.zeros((num_themes, 3), dtype=float)
        medians_arr = np.zeros((num_themes, 3), dtype=float)
        mins_arr = np.zeros((num_themes, 3), dtype=float)
        maxs_arr = np.zeros((num_themes, 3), dtype=float)
        max_min_diff_arr = np.zeros((num_themes, 3), dtype=float)

        plane_arr = np.zeros((num_themes, 7), dtype=float)
        hue_prob_arr = -99.0 * np.ones((num_themes, hue_feat_len), dtype=float)

        for i in range(num_themes):
            num_colors_i = num_colors_list[i]
            if name == "chsv":
                col = chsv_mats[i]
            elif name == "lab":
                col = lab_mats[i]
            elif name == "hsv":
                col = hsv_mats[i]
            else:
                col = rgb_mats[i]

            col = col[:, :num_colors_i]
            color_arr[i, : 3 * num_colors_i] = col.reshape(-1, order="F")

            # Adjacent color differences along each channel. For HSV we treat
            # hue as circular when both colors are sufficiently saturated/bright.
            diffs = np.zeros((3, max(num_colors_i - 1, 1)), dtype=float)
            for j in range(1, num_colors_i):
                if name == "hsv":
                    hsv_curr = hsv_mats[i][:, :num_colors_i]
                    min_sat_val = min(
                        np.min(hsv_curr[1, j - 1 : j + 1]),
                        np.min(hsv_curr[2, j - 1 : j + 1]),
                    )
                    if min_sat_val >= sat_val_thresh:
                        pts = np.sort([col[0, j], col[0, j - 1]])
                        diffs[0, j - 1] = min(
                            pts[1] - pts[0],
                            1.0 - (pts[1] - pts[0]),
                        )
                else:
                    diffs[0, j - 1] = col[0, j] - col[0, j - 1]

                diffs[1, j - 1] = col[1, j] - col[1, j - 1]
                diffs[2, j - 1] = col[2, j] - col[2, j - 1]

            diff_arr[i, : 3 * (num_colors_i - 1)] = np.concatenate(
                [
                    diffs[0, : num_colors_i - 1],
                    diffs[1, : num_colors_i - 1],
                    diffs[2, : num_colors_i - 1],
                ]
            )

            num_diffs = num_colors_i - 1
            if num_diffs > 0:
                sorted_diff_arr[i, :num_diffs] = np.sort(diffs[0, :num_diffs])[::-1]
                sorted_diff_arr[i, num_diffs : 2 * num_diffs] = np.sort(diffs[1, :num_diffs])[::-1]
                sorted_diff_arr[i, 2 * num_diffs : 3 * num_diffs] = np.sort(diffs[2, :num_diffs])[::-1]

            means_arr[i, :] = np.mean(col, axis=1)
            stddevs_arr[i, :] = np.std(col, axis=1, ddof=0)
            medians_arr[i, :] = np.median(col, axis=1)
            mins_arr[i, :] = np.min(col, axis=1)
            maxs_arr[i, :] = np.max(col, axis=1)
            max_min_diff_arr[i, :] = maxs_arr[i, :] - mins_arr[i, :]

            if name != "hsv":
                X = col.T
                normal, pct_explained, _, sse = _get_plane_features(X)
                plane_arr[i, 0:3] = normal
                plane_arr[i, 3:6] = pct_explained
                plane_arr[i, 6] = sse
            else:
                hue_prob_arr[i, :] = _get_hue_prob_features(col, sat_val_thresh, hue_probs)

            order = np.argsort(col[2, :])
            col_sorted = col[:, order]
            sorted_col_arr[i, : 3 * num_colors_i] = col_sorted.reshape(-1, order="F")

        all_features[f"{name}Col"] = color_arr
        all_features[f"{name}SortedCol"] = sorted_col_arr
        all_features[f"{name}Diff"] = diff_arr
        all_features[f"{name}SortedDiff"] = sorted_diff_arr
        all_features[f"{name}Mean"] = means_arr
        all_features[f"{name}StdDev"] = stddevs_arr
        all_features[f"{name}Median"] = medians_arr
        all_features[f"{name}Max"] = maxs_arr
        all_features[f"{name}Min"] = mins_arr
        all_features[f"{name}MaxMinDiff"] = max_min_diff_arr

        if name != "hsv":
            all_features[f"{name}Plane"] = plane_arr
        else:
            # Some palettes may have no “visible” colors; those rows remain at
            # the sentinel value -99.0. We replace them with (max + ε) per
            # column so that:
            #   * all entries are finite for downstream stats, and
            #   * the relative ordering (small vs large probabilities) is kept.
            for col_idx in range(hue_prob_arr.shape[1]):
                col_vals = hue_prob_arr[:, col_idx]
                mask_neg = col_vals == -99.0
                if np.any(~mask_neg):
                    col_max = float(col_vals[~mask_neg].max())
                    col_vals[mask_neg] = col_max + 0.0001
                    hue_prob_arr[:, col_idx] = col_vals
            all_features[f"{name}HueProb"] = hue_prob_arr

    return all_features, feature_names, num_themes, rgbs, labs


# ---------------------------------------------------------------------------
# DataFrame-level feature helpers and pruning
# ---------------------------------------------------------------------------

def add_palette_features_to_df(
    df: pd.DataFrame, max_rows: Optional[int] = None, aux_base_path: str = "data/data"
) -> pd.DataFrame:
    """
    Compute color-palette features (Python port of createFeaturesFromData.m)
    for each row of the MTurk dataset and return a new DataFrame with features
    appended as additional columns.

    Args:
        df: DataFrame created by loadData, with color1_r ... color5_b columns.
        max_rows: Optional limit on number of rows to process.
        aux_base_path: Directory containing hueProbsRGB.mat and kulerX.mat.

    Returns:
        New DataFrame where each original row also has the engineered
        color features as extra columns.
    """
    color_blocks: List[np.ndarray] = []
    for idx in range(1, 6):
        cols = [f"color{idx}_r", f"color{idx}_g", f"color{idx}_b"]
        if not all(c in df.columns for c in cols):
            raise KeyError(f"Expected color columns {cols} in DataFrame.")
        color_blocks.append(df[cols].to_numpy())

    data_arr = np.stack(color_blocks, axis=1)

    all_features, feature_names, num_themes, _, _ = create_features_from_data_array(
        data_arr, max_features=max_rows, aux_base_path=aux_base_path
    )

    feature_cols: Dict[str, np.ndarray] = {}
    for group_name, arr in all_features.items():
        names = feature_names.get(group_name)
        if names is None or len(names) != arr.shape[1]:
            names = [f"{group_name}_{i}" for i in range(arr.shape[1])]
        for col_idx, col_name in enumerate(names):
            if col_name in feature_cols:
                col_name = f"{col_name}_{group_name}"
            feature_cols[col_name] = arr[:, col_idx]

    feats_df = pd.DataFrame(feature_cols)

    base_df = df.iloc[:num_themes].reset_index(drop=True)
    feats_df = feats_df.iloc[:num_themes].reset_index(drop=True)
    return pd.concat([base_df, feats_df], axis=1)


def select_top_mturk_features(
    df: pd.DataFrame,
    weights_csv_path: str = "data/weights.csv",
    k: int = TOP_K_MTURK_FEATURES,
) -> pd.DataFrame:
    """
    Keep only the top-k Mturk-weighted feature columns (plus core metadata).

    The weights file is expected to have columns:
        Feature, Kuler, ColorLovers, Mturk
    and feature names that match the engineered feature column names
    (e.g., 'chsv-D1-C1', 'labMean-D1', 'hsvHueProbMean', ...).
    """
    if k is None or k <= 0:
        return df

    try:
        weights_df = pd.read_csv(weights_csv_path)
    except FileNotFoundError:
        # If no weights file is available, return the original dataframe.
        return df

    if "Feature" not in weights_df.columns or "Mturk" not in weights_df.columns:
        return df

    weights_df["Feature"] = weights_df["Feature"].astype(str).str.strip()
    weights_df = weights_df.dropna(subset=["Mturk"])

    if weights_df.empty:
        return df

    weights_df["abs_weight"] = weights_df["Mturk"].abs()
    # Keep only features that actually exist as columns in df
    weights_df = weights_df[weights_df["Feature"].isin(df.columns)]
    if weights_df.empty:
        return df

    top_features = (
        weights_df.sort_values("abs_weight", ascending=False)["Feature"].head(k).tolist()
    )

    # Always keep core metadata/target columns if present
    core_cols = [c for c in ["ids", "names", "targets", "userNormalizedTargets"] if c in df.columns]
    selected_cols = core_cols + [c for c in top_features if c not in core_cols]

    # Some rows may have been truncated during feature creation; keep all rows, only trim columns.
    return df.loc[:, selected_cols]


def prune_highly_correlated_features(
    df: pd.DataFrame,
    threshold: float = 0.95,
) -> pd.DataFrame:
    """
    Drop one feature from any pair whose absolute correlation exceeds `threshold`.

    Core metadata/target columns ('ids', 'names', 'targets', 'userNormalizedTargets')
    are always retained and are not candidates for dropping.
    """
    core_cols = [c for c in ["ids", "names", "targets", "userNormalizedTargets"] if c in df.columns]
    feature_cols = [c for c in df.columns if c not in core_cols]

    if len(feature_cols) <= 1:
        print("\nFeature pruning: fewer than two feature columns; nothing to drop.")
        return df

    # Compute absolute correlation matrix for feature columns only
    corr = df[feature_cols].corr().abs()
    # Use the upper triangle so each (i, j) pair is considered only once
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))

    to_drop = set()
    drop_pairs: List[Tuple[str, str, float]] = []  # (dropped, kept, corr)

    for col in feature_cols:
        if col in to_drop:
            continue
        # Find features highly correlated with `col`
        high_corr = upper.index[upper[col] > threshold].tolist()
        for other in high_corr:
            if other not in to_drop:
                to_drop.add(other)
                drop_pairs.append((other, col, float(upper.loc[other, col])))

    if not drop_pairs:
        print(f"\nFeature pruning: no pairs with |corr| > {threshold}; nothing dropped.")
        return df

    print(f"\nFeature pruning (|corr| > {threshold}):")
    for dropped, kept, corr_val in drop_pairs:
        print(f"  Dropped '{dropped}' (kept '{kept}'), corr={corr_val:.3f}")

    pruned_df = df.drop(columns=list(to_drop))
    print(f"Feature pruning: dropped {len(to_drop)} of {len(feature_cols)} feature columns.")
    return pruned_df


# ---------------------------------------------------------------------------
# Model training utilities
# ---------------------------------------------------------------------------

def trainRatingBaseline(
    dfWithFeatures: pd.DataFrame,
    featureCols: List[str],
    testSize: float = 0.2,
    randomState: int = 0,
):
    """
    Train a rating-based baseline model to predict userNormalizedTargets from palette features.

    Steps:
        - Split palettes into train/test.
        - Scale features with StandardScaler.
        - Train a LASSO (L1) regression model.
        - Compute RMSE on the test set.

    Returns:
        model: fitted sklearn Lasso instance.
        scaler: fitted StandardScaler instance.
        X_train_scaled, X_test_scaled, y_train, y_test: train/test splits (features are scaled).
        train_rmse: float, RMSE on the training set.
        test_rmse: float, RMSE on the test set.
    """
    if "userNormalizedTargets" not in dfWithFeatures.columns:
        raise KeyError("Column 'userNormalizedTargets' not found in dfWithFeatures.")

    missing = [c for c in featureCols if c not in dfWithFeatures.columns]
    if missing:
        raise KeyError(f"The following feature columns are missing in dfWithFeatures: {missing}")

    # Extract features and target
    X = dfWithFeatures[featureCols].to_numpy(dtype=float)
    y = dfWithFeatures["userNormalizedTargets"].to_numpy(dtype=float)

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=testSize, random_state=randomState
    )

    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # LASSO regression baseline
    model = Lasso(alpha=0.001, max_iter=10000, random_state=randomState)
    model.fit(X_train_scaled, y_train)

    # Evaluate on train and test sets (RMSE)
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    train_rmse = float(np.sqrt(mean_squared_error(y_train, y_train_pred)))
    test_rmse = float(np.sqrt(mean_squared_error(y_test, y_test_pred)))

    return (
        model,
        scaler,
        X_train_scaled,
        X_test_scaled,
        y_train,
        y_test,
        train_rmse,
        test_rmse,
    )


def generateSyntheticPairs(
    train: pd.DataFrame,
    nPairs: int = 100000,
    delta: float = 0.2,
    randomState: int = 0,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Randomly sample palette pairs from a training dataframe and generate preference labels.

    Pairs where |userNormalizedTargets_i - userNormalizedTargets_j| < delta are skipped.

    Args:
        train: DataFrame containing userNormalizedTargets and palette feature columns.
        nPairs: Target number of synthetic pairs to generate.
        delta: Minimum absolute difference in userNormalizedTargets to keep a pair.
        randomState: Seed for the random number generator.

    Returns:
        Xi: np.ndarray of shape (n_kept, n_features), features for item i.
        Xj: np.ndarray of shape (n_kept, n_features), features for item j.
        yPairs: np.ndarray of shape (n_kept,), with labels:
                1 if userNormalizedTargets_i > userNormalizedTargets_j else 0.
    """
    if "userNormalizedTargets" not in train.columns:
        raise KeyError("Column 'userNormalizedTargets' not found in training dataframe.")

    feature_cols = [
        c for c in train.columns if c not in ["ids", "names", "targets", "userNormalizedTargets"]
    ]
    if not feature_cols:
        raise ValueError("No feature columns found in training dataframe.")

    n = len(train)
    if n < 2:
        raise ValueError("Need at least two palettes in training data to form pairs.")

    X_all = train[feature_cols].to_numpy(dtype=float)
    y_all = train["userNormalizedTargets"].to_numpy(dtype=float)

    rng = np.random.default_rng(randomState)

    Xi_list: List[np.ndarray] = []
    Xj_list: List[np.ndarray] = []
    y_list: List[np.ndarray] = []

    total_sampled = 0
    target_pairs = int(nPairs)
    # Oversample in batches to efficiently filter by delta.
    batch_size = min(max(target_pairs * 2, 1000), n * n)

    while sum(len(b) for b in y_list) < target_pairs and total_sampled < target_pairs * 20:
        size = batch_size
        idx_i = rng.integers(0, n, size=size)
        idx_j = rng.integers(0, n, size=size)

        # Exclude self-pairs
        mask = idx_i != idx_j
        idx_i = idx_i[mask]
        idx_j = idx_j[mask]

        if idx_i.size == 0:
            total_sampled += size
            continue

        diff = y_all[idx_i] - y_all[idx_j]
        keep = np.abs(diff) >= delta
        idx_i = idx_i[keep]
        idx_j = idx_j[keep]
        diff = diff[keep]

        total_sampled += size

        if idx_i.size == 0:
            continue

        remaining = target_pairs - sum(len(b) for b in y_list)
        if remaining <= 0:
            break

        if idx_i.size > remaining:
            idx_i = idx_i[:remaining]
            idx_j = idx_j[:remaining]
            diff = diff[:remaining]

        Xi_list.append(X_all[idx_i])
        Xj_list.append(X_all[idx_j])
        y_list.append((diff > 0).astype(int))

    if not y_list:
        print(
            f"\nSynthetic pair generation: no valid pairs found with "
            f"|delta| >= {delta}. Returning empty arrays."
        )
        return (
            np.empty((0, len(feature_cols)), dtype=float),
            np.empty((0, len(feature_cols)), dtype=float),
            np.empty((0,), dtype=int),
        )

    Xi = np.vstack(Xi_list)
    Xj = np.vstack(Xj_list)
    yPairs = np.concatenate(y_list)

    n_kept = Xi.shape[0]

    print("\nSynthetic pair generation summary:")
    print(f"  Requested pairs: {target_pairs}")
    print(f"  Total candidate pairs sampled: {total_sampled}")
    print(f"  Pairs kept after |delta| >= {delta}: {n_kept}")
    if n_kept < target_pairs:
        print(
            "  Note: fewer pairs than requested were generated; "
            "consider lowering delta or increasing training data size."
        )

    if n_kept > 0:
        prop_1 = float(yPairs.mean())
        prop_0 = 1.0 - prop_1
        print(f"  Proportion of label 1: {prop_1:.3f}")
        print(f"  Proportion of label 0: {prop_0:.3f}")

    return Xi, Xj, yPairs


# ---------------------------------------------------------------------------
# Bradley–Terry model training and evaluation
# ---------------------------------------------------------------------------

def trainBtLogisticModel(
    Xi: np.ndarray,
    Xj: np.ndarray,
    yPairs: np.ndarray,
    C: float = 1.0,
    max_iter: int = 1000,
    randomState: int = 0,
) -> Tuple[LogisticRegression, StandardScaler]:
    """
    Train a Bradley–Terry style logistic regression model on synthetic pairwise data.

    The model operates on feature differences (Xi - Xj) to predict yPairs.

    Args:
        Xi: Array of shape (n_pairs, n_features), features for item i.
        Xj: Array of shape (n_pairs, n_features), features for item j.
        yPairs: Array of shape (n_pairs,), with labels 1 if i is preferred to j, else 0.
        C: Inverse of regularization strength for LogisticRegression (L2 penalty).
        max_iter: Maximum number of iterations for the solver.
        randomState: Random seed for the LogisticRegression model.

    Returns:
        model: Fitted sklearn LogisticRegression instance.
        scaler: Fitted StandardScaler instance used on (Xi - Xj).
    """
    Xi = np.asarray(Xi, dtype=float)
    Xj = np.asarray(Xj, dtype=float)
    yPairs = np.asarray(yPairs).astype(int)

    if Xi.shape != Xj.shape:
        raise ValueError(f"Xi and Xj must have the same shape, got {Xi.shape} vs {Xj.shape}.")
    if Xi.shape[0] != yPairs.shape[0]:
        raise ValueError(
            f"Number of pairs in Xi/Xj and yPairs must match, "
            f"got {Xi.shape[0]} vs {yPairs.shape[0]}."
        )
    if Xi.shape[0] == 0:
        raise ValueError("No pairwise data provided: Xi has zero rows.")

    # Construct feature differences for Bradley–Terry style modeling
    X_diff = Xi - Xj

    # Standardize features
    scaler = StandardScaler()
    X_diff_scaled = scaler.fit_transform(X_diff)

    # L2-regularized logistic regression
    model = LogisticRegression(
        penalty="l2",
        C=C,
        max_iter=max_iter,
        solver="lbfgs",
        random_state=randomState,
    )
    model.fit(X_diff_scaled, yPairs)

    return model, scaler


def evaluateBtModelOnTestPairs(
    model: LogisticRegression,
    scaler: StandardScaler,
    XTest: np.ndarray,
    yTest: np.ndarray,
    nPairs: int = 20000,
    delta: float = 0.2,
) -> Tuple[float, float]:
    """
    Evaluate a Bradley–Terry logistic model on synthetic test pairs.

    Randomly samples pairs from the test set, filters out pairs with
    |y_i - y_j| < delta, and computes:
        - Pairwise accuracy (fraction of correctly ordered pairs).
        - ROC AUC based on predicted pairwise probabilities.

    Args:
        model: Fitted LogisticRegression model from trainBtLogisticModel.
        scaler: StandardScaler fitted on (Xi - Xj) during training.
        XTest: Array of shape (n_samples, n_features) with test palette features.
        yTest: Array of shape (n_samples,) with test userNormalizedTargets.
        nPairs: Target number of test pairs to sample.
        delta: Minimum absolute rating difference required to keep a pair.

    Returns:
        pair_accuracy: float, pairwise accuracy over sampled pairs.
        pair_auc: float, ROC AUC over sampled pairs (np.nan if undefined).
    """
    XTest = np.asarray(XTest, dtype=float)
    yTest = np.asarray(yTest, dtype=float)

    if XTest.ndim != 2:
        raise ValueError(f"XTest must be 2D, got shape {XTest.shape}.")
    if XTest.shape[0] != yTest.shape[0]:
        raise ValueError(
            f"Number of samples in XTest and yTest must match, "
            f"got {XTest.shape[0]} vs {yTest.shape[0]}."
        )
    if XTest.shape[0] < 2:
        raise ValueError("Need at least two test samples to form pairs.")

    n = XTest.shape[0]
    target_pairs = int(nPairs)
    rng = np.random.default_rng(0)

    Xdiff_list: List[np.ndarray] = []
    ypair_list: List[np.ndarray] = []
    total_sampled = 0

    # Oversample in batches so we can filter by delta efficiently
    batch_size = min(max(target_pairs * 2, 1000), n * n)

    while sum(len(b) for b in ypair_list) < target_pairs and total_sampled < target_pairs * 20:
        size = batch_size
        idx_i = rng.integers(0, n, size=size)
        idx_j = rng.integers(0, n, size=size)

        # Exclude self-pairs
        mask = idx_i != idx_j
        idx_i = idx_i[mask]
        idx_j = idx_j[mask]

        if idx_i.size == 0:
            total_sampled += size
            continue

        diff_y = yTest[idx_i] - yTest[idx_j]
        keep = np.abs(diff_y) >= delta
        idx_i = idx_i[keep]
        idx_j = idx_j[keep]
        diff_y = diff_y[keep]

        total_sampled += size

        if idx_i.size == 0:
            continue

        remaining = target_pairs - sum(len(b) for b in ypair_list)
        if remaining <= 0:
            break

        if idx_i.size > remaining:
            idx_i = idx_i[:remaining]
            idx_j = idx_j[:remaining]
            diff_y = diff_y[:remaining]

        Xdiff_list.append(XTest[idx_i] - XTest[idx_j])
        ypair_list.append((diff_y > 0).astype(int))

    if not ypair_list:
        print(
            f"\nBT test evaluation: no valid pairs found with "
            f"|delta| >= {delta}. Returning NaN metrics."
        )
        return float("nan"), float("nan")

    X_diff = np.vstack(Xdiff_list)
    yPairs = np.concatenate(ypair_list)
    n_kept = X_diff.shape[0]

    X_diff_scaled = scaler.transform(X_diff)
    proba = model.predict_proba(X_diff_scaled)[:, 1]
    y_pred = (proba >= 0.5).astype(int)

    pair_accuracy = float((y_pred == yPairs).mean())

    if yPairs.min() == yPairs.max():
        pair_auc = float("nan")
        print(
            "\nBT test evaluation: only one class present in yPairs; "
            "ROC AUC is undefined (set to NaN)."
        )
    else:
        pair_auc = float(roc_auc_score(yPairs, proba))

    print("\nBT test pair evaluation summary:")
    print(f"  Requested test pairs: {target_pairs}")
    print(f"  Total candidate test pairs sampled: {total_sampled}")
    print(f"  Test pairs kept after |delta| >= {delta}: {n_kept}")
    print(f"  Pairwise accuracy: {pair_accuracy:.4f}")
    if np.isfinite(pair_auc):
        print(f"  Pairwise ROC AUC: {pair_auc:.4f}")
    else:
        print("  Pairwise ROC AUC: NaN (undefined due to single-class labels).")

    return pair_accuracy, pair_auc


# ---------------------------------------------------------------------------
# Experimental orchestration, evaluation, and visualization
# ---------------------------------------------------------------------------

def run_experiment_for_seed(
    df_final: pd.DataFrame,
    feature_cols: List[str],
    seed: int,
) -> Dict[str, float]:
    """
    Run LASSO rating baseline + BT logistic pairwise model for a single random seed.

    Returns a dict with key metrics for later aggregation (mean ± std, significance tests).
    """
    (
        model,
        scaler,
        X_train_scaled,
        X_test_scaled,
        y_train,
        y_test,
        train_rmse,
        test_rmse,
    ) = trainRatingBaseline(df_final, feature_cols, randomState=seed)

    X_all = df_final[feature_cols].to_numpy(dtype=float)
    y_all = df_final["userNormalizedTargets"].to_numpy(dtype=float)

    X_train_bt, X_test_bt, y_train_bt, y_test_bt = train_test_split(
        X_all, y_all, test_size=0.2, random_state=seed
    )

    train_bt_df = pd.DataFrame(X_train_bt, columns=feature_cols)
    train_bt_df["userNormalizedTargets"] = y_train_bt

    Xi_train, Xj_train, yPairs_train = generateSyntheticPairs(
        train_bt_df, nPairs=10000, delta=0.2, randomState=seed
    )

    if Xi_train.shape[0] == 0:
        return {
            "seed": seed,
            "lasso_test_rmse": test_rmse,
            "bt_pair_acc": np.nan,
            "bt_pair_auc": np.nan,
            "lasso_pair_acc": np.nan,
            "lasso_pair_auc": np.nan,
        }

    bt_model, bt_scaler = trainBtLogisticModel(Xi_train, Xj_train, yPairs_train)

    # Shared test pairs for BT and LASSO
    n_test = X_test_bt.shape[0]
    target_pairs = 20000
    rng = np.random.default_rng(seed)

    idx_i_list: List[np.ndarray] = []
    idx_j_list: List[np.ndarray] = []
    ypair_list: List[np.ndarray] = []
    total_sampled = 0

    batch_size = min(max(target_pairs * 2, 1000), n_test * n_test)

    while sum(len(b) for b in ypair_list) < target_pairs and total_sampled < target_pairs * 20:
        size = batch_size
        idx_i = rng.integers(0, n_test, size=size)
        idx_j = rng.integers(0, n_test, size=size)

        mask = idx_i != idx_j
        idx_i = idx_i[mask]
        idx_j = idx_j[mask]

        if idx_i.size == 0:
            total_sampled += size
            continue

        diff_y = y_test_bt[idx_i] - y_test_bt[idx_j]
        keep = np.abs(diff_y) >= 0.2
        idx_i = idx_i[keep]
        idx_j = idx_j[keep]
        diff_y = diff_y[keep]

        total_sampled += size

        if idx_i.size == 0:
            continue

        remaining = target_pairs - sum(len(b) for b in ypair_list)
        if remaining <= 0:
            break

        if idx_i.size > remaining:
            idx_i = idx_i[:remaining]
            idx_j = idx_j[:remaining]
            diff_y = diff_y[:remaining]

        idx_i_list.append(idx_i)
        idx_j_list.append(idx_j)
        ypair_list.append((diff_y > 0).astype(int))

    if not ypair_list:
        return {
            "seed": seed,
            "lasso_test_rmse": test_rmse,
            "bt_pair_acc": np.nan,
            "bt_pair_auc": np.nan,
            "lasso_pair_acc": np.nan,
            "lasso_pair_auc": np.nan,
        }

    idx_i_all = np.concatenate(idx_i_list)
    idx_j_all = np.concatenate(idx_j_list)
    yPairs_test = np.concatenate(ypair_list)

    X_diff_bt = X_test_bt[idx_i_all] - X_test_bt[idx_j_all]
    X_diff_bt_scaled = bt_scaler.transform(X_diff_bt)
    proba_bt = bt_model.predict_proba(X_diff_bt_scaled)[:, 1]
    y_pred_bt = (proba_bt >= 0.5).astype(int)

    bt_pair_acc = float((y_pred_bt == yPairs_test).mean())
    if yPairs_test.min() == yPairs_test.max():
        bt_pair_auc = np.nan
    else:
        bt_pair_auc = float(roc_auc_score(yPairs_test, proba_bt))

    X_test_scaled_for_lasso = scaler.transform(X_test_bt)
    scores_i = model.predict(X_test_scaled_for_lasso[idx_i_all])
    scores_j = model.predict(X_test_scaled_for_lasso[idx_j_all])
    score_diff = scores_i - scores_j
    y_pred_lasso_pairs = (score_diff > 0).astype(int)
    lasso_pair_acc = float((y_pred_lasso_pairs == yPairs_test).mean())

    if yPairs_test.min() == yPairs_test.max():
        lasso_pair_auc = np.nan
    else:
        lasso_pair_auc = float(roc_auc_score(yPairs_test, score_diff))

    return {
        "seed": seed,
        "lasso_test_rmse": test_rmse,
        "bt_pair_acc": bt_pair_acc,
        "bt_pair_auc": bt_pair_auc,
        "lasso_pair_acc": lasso_pair_acc,
        "lasso_pair_auc": lasso_pair_auc,
    }


def summarizeRatings(df: pd.DataFrame) -> None:
    """
    Print summary statistics and plot a histogram for targets.
    """
    if "targets" not in df.columns:
        raise KeyError("Column 'targets' not found in DataFrame.")

    meanRating = df["targets"].mean()
    stdRating = df["targets"].std()

    print("Mean of targets:", meanRating)
    print("Std of targets:", stdRating)

    plt.figure(figsize=(6, 4))
    plt.hist(df["targets"], bins=20, edgecolor="black")
    plt.xlabel("targets")
    plt.ylabel("Frequency")
    plt.title("Distribution of targets")
    plt.tight_layout()
    # Save to file so the plot is available even in non-interactive environments.
    plt.savefig(os.path.join(OUTPUT_DIR, "targets_hist.png"))
    # plt.show()


def summarizeUserNormalizedTargets(df: pd.DataFrame) -> None:
    """
    Print summary statistics and plot a histogram for userNormalizedTargets.
    """
    colName = "userNormalizedTargets"
    if colName not in df.columns:
        raise KeyError(f"Column '{colName}' not found in DataFrame.")

    meanVal = df[colName].mean()
    stdVal = df[colName].std()

    print(f"Mean of {colName}:", meanVal)
    print(f"Std of {colName}:", stdVal)

    plt.figure(figsize=(6, 4))
    plt.hist(df[colName], bins=20, edgecolor="black")
    plt.xlabel(colName)
    plt.ylabel("Frequency")
    plt.title(f"Distribution of {colName}")
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, "user_normalized_targets_hist.png"))
    # plt.show()


def visualize_palette_pair(
    df: pd.DataFrame,
    global_i: int,
    global_j: int,
    title: str,
    filename: str,
) -> None:
    """
    Helper to visualize a pair of palettes (5 colors each) and save to disk.
    """
    row_i = df.iloc[global_i]
    row_j = df.iloc[global_j]

    palette_i = [
        (row_i[f"color{k}_r"], row_i[f"color{k}_g"], row_i[f"color{k}_b"])
        for k in range(1, 6)
    ]
    palette_j = [
        (row_j[f"color{k}_r"], row_j[f"color{k}_g"], row_j[f"color{k}_b"])
        for k in range(1, 6)
    ]

    fig, axes = plt.subplots(2, 5, figsize=(5, 2))
    for ax_row, palette in zip(axes, [palette_i, palette_j]):
        for ax, (r, g, b) in zip(ax_row, palette):
            ax.imshow([[(r, g, b)]])
            ax.axis("off")

    fig.suptitle(title)
    plt.tight_layout()
    # Always save palette visualizations inside the shared OUTPUT_DIR
    plt.savefig(os.path.join(OUTPUT_DIR, filename))
    plt.close(fig)


def prepare_data() -> pd.DataFrame:
    """
    Load the raw MTurk dataset and print basic descriptive statistics.
    """
    df = loadData()

    print("Base dataset shape:", df.shape)
    print("Base columns:", list(df.columns))
    print("\nBase head:")
    print(df.head())

    print("\nAll base columns describe():")
    print(df.describe(include="all"))

    if "targets" in df.columns:
        print("\nBase targets describe():")
        print(df["targets"].describe())
    else:
        print("\nColumn 'targets' not found; please adjust loader.")

    summarizeRatings(df)

    if "userNormalizedTargets" in df.columns:
        print("\nBase userNormalizedTargets describe():")
        print(df["userNormalizedTargets"].describe())
        summarizeUserNormalizedTargets(df)

    return df


def build_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    """
    Construct engineered palette features, drop raw RGB, and prune correlations.
    """
    df_with_features = add_palette_features_to_df(df, aux_base_path="data/data")

    raw_rgb_cols = [f"color{i}_{ch}" for i in range(1, 6) for ch in ["r", "g", "b"]]
    df_with_features = df_with_features.drop(columns=raw_rgb_cols, errors="ignore")

    df_final = prune_highly_correlated_features(df_with_features, threshold=0.95)

    print("\nFinal dataframe after feature engineering and correlation pruning:")
    print("Final shape:", df_final.shape)
    print("Number of columns:", len(df_final.columns))
    print("Final columns:")
    print(list(df_final.columns))
    print("\nFinal head:")
    print(df_final.head())

    feature_cols = [
        c for c in df_final.columns if c not in ["ids", "names", "targets", "userNormalizedTargets"]
    ]
    return df_final, feature_cols


def train_and_evaluate_single_seed(
    df: pd.DataFrame, df_final: pd.DataFrame, feature_cols: List[str]
) -> None:
    """
    Run the end-to-end single-seed experiment.

    This function ties together:
        - training and evaluating the LASSO rating baseline,
        - training the BT logistic model on synthetic pairs,
        - evaluating both models on a shared set of test pairs,
        - generating diagnostic plots and palette visualizations, and
        - performing error analysis and feature-importance comparison.
    """
    # -----------------------------------------------------------------------
    # 1. Train and evaluate rating-based baseline on the final feature set
    # -----------------------------------------------------------------------
    if feature_cols:
        (
            model,
            scaler,
            X_train_scaled,
            X_test_scaled,
            y_train,
            y_test,
            train_rmse,
            test_rmse,
        ) = trainRatingBaseline(df_final, feature_cols)
        print("\nRating-based baseline (LASSO) performance:")
        print(f"  Number of training examples: {len(y_train)}")
        print(f"  Number of test examples: {len(y_test)}")
        print(f"  Number of feature columns: {len(feature_cols)}")
        print(f"  Train RMSE on userNormalizedTargets: {train_rmse:.4f}")
        print(f"  Test RMSE on userNormalizedTargets: {test_rmse:.4f}")

        # Simple overfitting check: flag when train error is much lower than test error.
        if train_rmse < 0.9 * test_rmse:
            print(
                "  Overfitting check: WARNING - model may be overfitting "
                "(train RMSE significantly lower than test RMSE)."
            )
        else:
                print(
                    "  Overfitting check: no strong evidence of overfitting "
                    "based on train/test RMSE."
                )

        # -------------------------------------------------------------------
        # 2. Rating-baseline diagnostics: scatter + calibration plots
        # -------------------------------------------------------------------
        # True-vs-predicted scatter for the held-out test split
        y_test_pred = model.predict(X_test_scaled)
        plt.figure(figsize=(5, 4))
        plt.scatter(y_test, y_test_pred, alpha=0.3)
        plt.plot(
            [y_test.min(), y_test.max()],
            [y_test.min(), y_test.max()],
            "r--",
            linewidth=1,
        )
        plt.xlabel("True userNormalizedTargets")
        plt.ylabel("Predicted userNormalizedTargets")
        plt.title("LASSO rating baseline: true vs predicted (test)")
        plt.tight_layout()
        plt.savefig(os.path.join(OUTPUT_DIR, "lasso_true_vs_pred_test.png"))
        plt.close()

        # Calibration-style plot: binned true vs. predicted means
        bins = np.linspace(y_test.min(), y_test.max(), 11)
        bin_indices = np.digitize(y_test, bins) - 1
        bin_centers = 0.5 * (bins[:-1] + bins[1:])
        true_means = []
        pred_means = []
        for b in range(len(bin_centers)):
            mask = bin_indices == b
            if np.any(mask):
                true_means.append(float(y_test[mask].mean()))
                pred_means.append(float(y_test_pred[mask].mean()))
            else:
                true_means.append(np.nan)
                pred_means.append(np.nan)
        plt.figure(figsize=(5, 4))
        plt.plot(bin_centers, true_means, "o-", label="True")
        plt.plot(bin_centers, pred_means, "s-", label="Predicted")
        plt.xlabel("Target bin center")
        plt.ylabel("Mean userNormalizedTargets")
        plt.title("LASSO rating baseline: calibration by target bins (test)")
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(OUTPUT_DIR, "lasso_calibration_test.png"))
        plt.close()

        # -------------------------------------------------------------------
        # 3. Train Bradley–Terry logistic model using synthetic pairs
        # -------------------------------------------------------------------
        print("\nTraining Bradley–Terry logistic model on synthetic pairs...")
        X_all = df_final[feature_cols].to_numpy(dtype=float)
        y_all = df_final["userNormalizedTargets"].to_numpy(dtype=float)
        indices_all = np.arange(df_final.shape[0])

        (
            X_train_bt,
            X_test_bt,
            y_train_bt,
            y_test_bt,
            idx_train_bt,
            idx_test_bt,
        ) = train_test_split(X_all, y_all, indices_all, test_size=0.2, random_state=0)

        train_bt_df = pd.DataFrame(X_train_bt, columns=feature_cols)
        train_bt_df["userNormalizedTargets"] = y_train_bt

        Xi_train, Xj_train, yPairs_train = generateSyntheticPairs(
            train_bt_df, nPairs=10000, delta=0.2, randomState=0
        )

        if Xi_train.shape[0] > 0:
            bt_model, bt_scaler = trainBtLogisticModel(Xi_train, Xj_train, yPairs_train)

            # ---------------------------------------------------------------
            # 4. Sample a shared set of test pairs from the BT test split
            #    (used for both BT and LASSO pairwise evaluation)
            # ---------------------------------------------------------------
            XTest = X_test_bt
            yTest = y_test_bt
            n_test = XTest.shape[0]
            target_pairs = 20000
            rng = np.random.default_rng(0)

            idx_i_list: List[np.ndarray] = []
            idx_j_list: List[np.ndarray] = []
            ypair_list: List[np.ndarray] = []
            total_sampled = 0

            batch_size = min(max(target_pairs * 2, 1000), n_test * n_test)

            while sum(len(b) for b in ypair_list) < target_pairs and total_sampled < target_pairs * 20:
                size = batch_size
                idx_i = rng.integers(0, n_test, size=size)
                idx_j = rng.integers(0, n_test, size=size)

                # Exclude self-pairs
                mask = idx_i != idx_j
                idx_i = idx_i[mask]
                idx_j = idx_j[mask]

                if idx_i.size == 0:
                    total_sampled += size
                    continue

                diff_y = yTest[idx_i] - yTest[idx_j]
                keep = np.abs(diff_y) >= 0.2
                idx_i = idx_i[keep]
                idx_j = idx_j[keep]
                diff_y = diff_y[keep]

                total_sampled += size

                if idx_i.size == 0:
                    continue

                remaining = target_pairs - sum(len(b) for b in ypair_list)
                if remaining <= 0:
                    break

                if idx_i.size > remaining:
                    idx_i = idx_i[:remaining]
                    idx_j = idx_j[:remaining]
                    diff_y = diff_y[:remaining]

                idx_i_list.append(idx_i)
                idx_j_list.append(idx_j)
                ypair_list.append((diff_y > 0).astype(int))

            if not ypair_list:
                print(
                    "\nBT / LASSO pairwise evaluation: no valid test pairs found "
                    "with |delta| >= 0.2. Skipping pairwise evaluation."
                )
                return

            idx_i_all = np.concatenate(idx_i_list)
            idx_j_all = np.concatenate(idx_j_list)
            yPairs_test = np.concatenate(ypair_list)
            n_kept = idx_i_all.shape[0]

            # ---------------------------------------------------------------
            # 5. Evaluate BT model on these shared test pairs
            # ---------------------------------------------------------------
            X_diff_bt = XTest[idx_i_all] - XTest[idx_j_all]
            X_diff_bt_scaled = bt_scaler.transform(X_diff_bt)
            proba_bt = bt_model.predict_proba(X_diff_bt_scaled)[:, 1]
            y_pred_bt = (proba_bt >= 0.5).astype(int)

            pair_acc_bt = float((y_pred_bt == yPairs_test).mean())
            if yPairs_test.min() == yPairs_test.max():
                pair_auc_bt = float("nan")
            else:
                pair_auc_bt = float(roc_auc_score(yPairs_test, proba_bt))

            print("\nBT test pair evaluation summary (shared pairs):")
            print(f"  Requested test pairs: {target_pairs}")
            print(f"  Total candidate test pairs sampled: {total_sampled}")
            print(f"  Test pairs kept after |delta| >= 0.2: {n_kept}")
            print(f"  Pairwise accuracy: {pair_acc_bt:.4f}")
            if np.isfinite(pair_auc_bt):
                print(f"  Pairwise ROC AUC: {pair_auc_bt:.4f}")
            else:
                print("  Pairwise ROC AUC: NaN (undefined due to single-class labels).")

            # Visualize the most certain BT comparisons (highest |p - 0.5|)
            # using original palettes, skipping cases where the model is over
            # 99% certain (p <= 0.01 or p >= 0.99).
            if n_kept > 0:
                certainty = np.abs(proba_bt - 0.5)
                valid_mask = (proba_bt > 0.005) & (proba_bt < 0.995)
                valid_indices = np.where(valid_mask)[0]

                if valid_indices.size == 0:
                    print(
                        "\nVisualizing most certain BT pairs: all probabilities are 0 or 1; "
                        "skipping visualization."
                    )
                else:
                    n_visualize = min(15, valid_indices.size)
                    sorted_valid = valid_indices[np.argsort(-certainty[valid_indices])]
                    top_idx = sorted_valid[:n_visualize]

                    print(
                        f"\nVisualizing {n_visualize} most certain BT test pairs "
                        f"(excluding p=0 or 1, saved as PNG files):"
                    )

                    for rank, pair_idx in enumerate(top_idx, start=1):
                        test_i = idx_i_all[pair_idx]
                        test_j = idx_j_all[pair_idx]
                        global_i = idx_test_bt[test_i]
                        global_j = idx_test_bt[test_j]

                        p_ij = float(proba_bt[pair_idx])
                        if p_ij >= 0.5:
                            direction = "i>j"
                            certainty_val = p_ij
                        else:
                            direction = "j>i"
                            certainty_val = 1.0 - p_ij

                        filename = f"bt_most_certain_pair_{rank}.png"
                        visualize_palette_pair(
                            df,
                            global_i,
                            global_j,
                            title=(
                                f"BT Correct Prediction: p(i>j)={p_ij:.5f}, "
                                f"true={yPairs_test[pair_idx]}"
                            ),
                            filename=filename,
                        )

                        print(
                            f"  Pair {rank}: saved '{filename}' "
                            f"(palette indices {global_i} vs {global_j}, "
                            f"direction={direction}, certainty={certainty_val:.5f})"
                        )

            # --- Evaluate LASSO rating baseline on the same test pairs ---
            # Here we reuse the same sampled test indices so that BT and LASSO
            # are directly comparable on identical comparisons.
            X_test_bt_scaled_for_lasso = scaler.transform(XTest)
            scores_i = model.predict(X_test_bt_scaled_for_lasso[idx_i_all])
            scores_j = model.predict(X_test_bt_scaled_for_lasso[idx_j_all])
            score_diff = scores_i - scores_j
            y_pred_lasso_pairs = (score_diff > 0).astype(int)
            pair_acc_lasso = float((y_pred_lasso_pairs == yPairs_test).mean())

            if yPairs_test.min() == yPairs_test.max():
                pair_auc_lasso = float("nan")
            else:
                pair_auc_lasso = float(roc_auc_score(yPairs_test, score_diff))

            print("\nLASSO rating baseline pairwise evaluation on same test pairs:")
            print(f"  Pairwise accuracy: {pair_acc_lasso:.4f}")
            if np.isfinite(pair_auc_lasso):
                print(f"  Pairwise ROC AUC: {pair_auc_lasso:.4f}")
            else:
                print("  Pairwise ROC AUC: NaN (undefined due to single-class labels).")

            # ------------------------------------------------------------
            # 6. Error analysis and additional diagnostics
            # ------------------------------------------------------------
            print("\nError analysis for BT and LASSO on test pairs:")

            # BT worst mistakes: highest-confidence misclassifications
            certainty_bt = np.where(proba_bt >= 0.5, proba_bt, 1.0 - proba_bt)
            mis_mask = y_pred_bt != yPairs_test
            mis_indices = np.where(mis_mask)[0]
            if mis_indices.size > 0:
                worst_order = mis_indices[np.argsort(-certainty_bt[mis_indices])]
                top_worst = worst_order[:3]
                print("  BT worst mistakes (highest certainty misclassified pairs):")
                for rank, idx in enumerate(top_worst, start=1):
                    test_i = idx_i_all[idx]
                    test_j = idx_j_all[idx]
                    global_i = idx_test_bt[test_i]
                    global_j = idx_test_bt[test_j]
                    p_ij = float(proba_bt[idx])
                    visualize_palette_pair(
                        df,
                        global_i,
                        global_j,
                        title=(
                            f"BT worst mistake: p(i>j)={p_ij:.5f}, "
                            f"true={yPairs_test[idx]}"
                        ),
                        filename=f"bt_worst_mistake_{rank}.png",
                    )
                    print(
                        f"    Mistake {rank}: indices {global_i} vs {global_j}, "
                        f"p(i>j)={p_ij:.5f}, true={yPairs_test[idx]}"
                    )
            else:
                print("  BT worst mistakes: none (no misclassified pairs in sample).")

            # BT borderline cases: most uncertain correct predictions
            correct_mask = y_pred_bt == yPairs_test
            correct_indices = np.where(correct_mask)[0]
            if correct_indices.size > 0:
                borderline_order = correct_indices[np.argsort(np.abs(proba_bt[correct_indices] - 0.5))]
                top_borderline = borderline_order[:3]
                print("  BT borderline correct pairs (closest to p=0.5):")
                for rank, idx in enumerate(top_borderline, start=1):
                    test_i = idx_i_all[idx]
                    test_j = idx_j_all[idx]
                    global_i = idx_test_bt[test_i]
                    global_j = idx_test_bt[test_j]
                    p_ij = float(proba_bt[idx])
                    print(
                        f"    Borderline {rank}: indices {global_i} vs {global_j}, "
                        f"p(i>j)={p_ij:.5f}, true={yPairs_test[idx]}"
                    )
            else:
                print("  BT borderline correct pairs: none.")

            # Disagreements between BT and LASSO
            disagree_mask = y_pred_bt != y_pred_lasso_pairs
            disagree_indices = np.where(disagree_mask)[0]
            if disagree_indices.size > 0:
                # Focus on strongest disagreements where both models are relatively confident
                combined_cert = certainty_bt * np.abs(score_diff)
                disagree_order = disagree_indices[np.argsort(-combined_cert[disagree_indices])]
                top_disagree = disagree_order[:3]
                print("  Strong BT vs LASSO disagreements:")
                for rank, idx in enumerate(top_disagree, start=1):
                    test_i = idx_i_all[idx]
                    test_j = idx_j_all[idx]
                    global_i = idx_test_bt[test_i]
                    global_j = idx_test_bt[test_j]
                    p_ij = float(proba_bt[idx])
                    sd = float(score_diff[idx])
                    print(
                        f"    Disagreement {rank}: indices {global_i} vs {global_j}, "
                        f"BT p(i>j)={p_ij:.5f}, LASSO score_diff={sd:.5f}, "
                        f"BT_pred={y_pred_bt[idx]}, LASSO_pred={y_pred_lasso_pairs[idx]}, "
                        f"true={yPairs_test[idx]}"
                    )
            else:
                print("  Strong BT vs LASSO disagreements: none in sampled pairs.")

            # ------------------------------------------------------------
            # 7. Compare feature importances between LASSO and BT logistic
            #    models (coefficient magnitudes)
            # ------------------------------------------------------------
            print("\nFeature-importance comparison between LASSO and BT logistic models (top 10 by |coef|):")
            n_top = min(20, len(feature_cols))

            # LASSO: coefficients aligned with feature_cols
            lasso_coef = np.asarray(getattr(model, "coef_", None), dtype=float).reshape(-1)
            lasso_importances = np.abs(lasso_coef)
            lasso_order = np.argsort(-lasso_importances)[:n_top]
            lasso_top = [(feature_cols[i], float(lasso_importances[i])) for i in lasso_order]

            # BT logistic: coefficients on feature differences, also aligned with feature_cols
            bt_coef = np.asarray(bt_model.coef_[0], dtype=float)
            bt_importances = np.abs(bt_coef)
            bt_order = np.argsort(-bt_importances)[:n_top]
            bt_top = [(feature_cols[i], float(bt_importances[i])) for i in bt_order]

            print("\n  LASSO top features:")
            for name, val in lasso_top:
                print(f"    {name}: |coef|={val:.4f}")

            print("\n  BT logistic top features:")
            for name, val in bt_top:
                print(f"    {name}: |coef|={val:.4f}")

            # Bar plots for feature importances (top n_top features)
            lasso_names = [name for name, _ in lasso_top]
            lasso_vals = [val for _, val in lasso_top]
            plt.figure(figsize=(8, 4))
            y_pos = np.arange(len(lasso_names))
            plt.barh(y_pos, lasso_vals)
            plt.yticks(y_pos, lasso_names)
            plt.gca().invert_yaxis()
            plt.xlabel("|coef|")
            plt.title("LASSO feature importances (top)")
            plt.tight_layout()
            plt.savefig(os.path.join(OUTPUT_DIR, "lasso_feature_importance_top.png"))
            plt.close()

            bt_names = [name for name, _ in bt_top]
            bt_vals = [val for _, val in bt_top]
            plt.figure(figsize=(8, 4))
            y_pos = np.arange(len(bt_names))
            plt.barh(y_pos, bt_vals)
            plt.yticks(y_pos, bt_names)
            plt.gca().invert_yaxis()
            plt.xlabel("|coef|")
            plt.title("BT logistic feature importances (top)")
            plt.tight_layout()
            plt.savefig(os.path.join(OUTPUT_DIR, "bt_feature_importance_top.png"))
            plt.close()

            lasso_set = {name for name, _ in lasso_top}
            bt_set = {name for name, _ in bt_top}

            overlap = sorted(lasso_set & bt_set)
            lasso_only = sorted(lasso_set - bt_set)
            bt_only = sorted(bt_set - lasso_set)

            print("\n  Overlap in top-10 features:")
            if overlap:
                for name in overlap:
                    print(f"    {name}")
            else:
                print("    (none)")

            print("\n  Features only in LASSO top-10:")
            if lasso_only:
                for name in lasso_only:
                    print(f"    {name}")
            else:
                print("    (none)")

            print("\n  Features only in BT logistic top-10:")
            if bt_only:
                for name in bt_only:
                    print(f"    {name}")
            else:
                print("    (none)")
    else:
        print("\nRating-based baseline: no feature columns available, skipping training.")


def run_multiseed_experiments(df_final: pd.DataFrame, feature_cols: List[str]) -> None:
    """
    Run the BT vs LASSO comparison across multiple seeds and report summary statistics
    and a paired t-test on pairwise accuracy.
    """
    seeds = [0, 1, 2, 3, 4]
    print("\n\nRunning repeated experiments for multiple seeds for statistical summary...")
    metrics_list = []
    for seed in seeds:
        print(f"\n  Running seed {seed}...")
        metrics = run_experiment_for_seed(df_final, feature_cols, seed)
        metrics_list.append(metrics)

    lasso_rmse_arr = np.array([m["lasso_test_rmse"] for m in metrics_list])
    bt_acc_arr = np.array([m["bt_pair_acc"] for m in metrics_list])
    lasso_acc_arr = np.array([m["lasso_pair_acc"] for m in metrics_list])

    print("\nAggregated metrics over seeds:")
    print(
        f"  LASSO test RMSE: mean={np.nanmean(lasso_rmse_arr):.4f}, "
        f"std={np.nanstd(lasso_rmse_arr):.4f}"
    )
    print(
        f"  BT pairwise accuracy: mean={np.nanmean(bt_acc_arr):.4f}, "
        f"std={np.nanstd(bt_acc_arr):.4f}"
    )
    print(
        f"  LASSO pairwise accuracy: mean={np.nanmean(lasso_acc_arr):.4f}, "
        f"std={np.nanstd(lasso_acc_arr):.4f}"
    )

    valid_mask = ~np.isnan(bt_acc_arr) & ~np.isnan(lasso_acc_arr)
    if valid_mask.sum() >= 2:
        t_stat, p_val = ttest_rel(bt_acc_arr[valid_mask], lasso_acc_arr[valid_mask])
        print(
            "\nSignificance test (paired t-test) for BT vs LASSO pairwise accuracy across seeds:"
        )
        print(f"  t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}")
    else:
        print(
            "\nSignificance test: insufficient valid seeds to run paired t-test "
            "for BT vs LASSO pairwise accuracy."
        )


def main() -> None:
    """
    Orchestrate the full experimental pipeline:
        1) Load and summarize data.
        2) Build engineered palette features.
        3) Train/evaluate models for a single seed (with visualization).
        4) Run multi-seed experiments and a paired t-test.
    """
    df = prepare_data()
    df_final, feature_cols = build_features(df)

    train_and_evaluate_single_seed(df, df_final, feature_cols)
    if feature_cols:
        run_multiseed_experiments(df_final, feature_cols)


if __name__ == "__main__":
    main()
